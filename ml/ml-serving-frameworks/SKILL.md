---
name: ml-serving-frameworks
description: Deploying models as high-performance, scalable microservices using specialized serving frameworks.
---

# ML Serving Frameworks

Moving a model from a notebook to production requires more than just an API endpoint.

## Key Challenges
- **Dynamic Batching**: Grouping requests to maximize GPU/CPU throughput.
- **Model Graphing**: Chaining pre-processing, inference, and post-processing steps.
- **Autoscaling**: Adjusting resources based on traffic.

## Frameworks
- **BentoML**: Packaging models as OCI-compliant containers.
- **Ray Serve**: A scalable model serving library built on Ray.
- **FastAPI**: For simple, lightweight model microservices.

## Best Practices
- **Inference Optimization**: Use ONNX or TensorRT to accelerate predictions.
- **Monitoring**: Track latency, throughput, and system health (CPU/Memory).

